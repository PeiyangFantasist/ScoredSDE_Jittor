{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7318e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jittor as jt\n",
    "jt.flags.use_cuda = 1\n",
    "def add_same_shape(a, b):\n",
    "    assert a.shape == b.shape, \"The two input tensors must have the same shape\"\n",
    "    return jt.code(\n",
    "        a.shape,  # Output Shape\n",
    "        a.dtype,  # Output Type\n",
    "        [a, b],   # Input Tensor List\n",
    "        cuda_src='''\n",
    "        __global__ void kernel_add(@ARGS_DEF) {\n",
    "            @PRECALC;\n",
    "            for (int i=0; i<in0_shape0; i++){\n",
    "                    @out(i) = @in0(i) + @in1(i);\n",
    "            }\n",
    "        }\n",
    "        kernel_add<<<32, 32>>>(@ARGS);\n",
    "        '''       \n",
    "    )             # Cuda Code\n",
    "\n",
    "jt.flags.use_cuda = 1\n",
    "a = jt.Var([1, 2, 3])\n",
    "b = jt.Var([3, 4, 5, 6])\n",
    "add_same_shape(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jittor as jt\n",
    "# 直接从op.py导入自定义函数（确保op.py与测试文件在同一目录）\n",
    "from op import jt_upfirdn2d_large\n",
    "\n",
    "# -------------------------- 1. 基础配置 --------------------------\n",
    "jt.flags.use_cuda = 1  # 开启CUDA（必须）\n",
    "jt.seed(42)            # 固定随机种子，结果可复现\n",
    "\n",
    "# -------------------------- 2. 生成测试数据（模拟CIFAR-10格式） --------------------------\n",
    "batch_size = 4          # 批量大小（可自定义）\n",
    "in_h, in_w = 32, 32     # CIFAR-10图像尺寸\n",
    "channels = 3            # RGB三通道\n",
    "kernel_size = 3         # 3×3均值滤波核（简单平滑效果）\n",
    "\n",
    "# 输入张量：(batch_size, in_h, in_w, channels)\n",
    "input_tensor = jt.randn(batch_size, in_h, in_w, channels)\n",
    "# 滤波核：(kernel_size, kernel_size)\n",
    "kernel = jt.ones(kernel_size, kernel_size) / (kernel_size ** 2)  # 均值权重\n",
    "\n",
    "# -------------------------- 3. 调用自定义函数（测试典型场景） --------------------------\n",
    "# 场景：上采样2倍 + 下采样2倍 + 1像素填充（最终尺寸不变，仅平滑）\n",
    "output_tensor = jt_upfirdn2d_large(\n",
    "    input=input_tensor,\n",
    "    kernel=kernel,\n",
    "    up_x=2, up_y=2,    # x/y方向上采样2倍\n",
    "    down_x=2, down_y=2, # x/y方向下采样2倍\n",
    "    pad_x0=1, pad_x1=1, # x方向左右各填1像素\n",
    "    pad_y0=1, pad_y1=1  # y方向上下各填1像素\n",
    ")\n",
    "\n",
    "# -------------------------- 4. 验证结果（简单检查） --------------------------\n",
    "print(\"=== 简化测试结果 ===\")\n",
    "print(f\"输入张量形状：{input_tensor.shape}\")\n",
    "print(f\"输出张量形状：{output_tensor.shape}\")\n",
    "print(f\"函数调用：{'成功' if output_tensor is not None else '失败'}\")\n",
    "\n",
    "# 额外检查：输出尺寸是否符合预期（此处预期与输入尺寸一致）\n",
    "expected_out_shape = (batch_size, in_h, in_w, channels)\n",
    "if output_tensor.shape == expected_out_shape:\n",
    "    print(f\"尺寸验证：通过（符合预期 {expected_out_shape}）\")\n",
    "else:\n",
    "    print(f\"尺寸验证：失败（预期 {expected_out_shape}，实际 {output_tensor.shape}）\")\n",
    "\n",
    "# 可选：打印部分输出值，确认数据合理性\n",
    "print(\"\\n输出张量前2个像素值（批量0、通道0）：\")\n",
    "output_tensor # 打印(0,0,0,0)和(0,0,1,0)位置的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64b4694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jt.Var([5], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jittor as jt\n",
    "a = jt.rand([5,2,1,4])\n",
    "def jt_int_mult(a):\n",
    "    x = jt.Var([1])\n",
    "    return jt.code(\n",
    "        x.shape,\n",
    "        jt.int32,\n",
    "        [a],\n",
    "        cpu_src=f'''\n",
    "                @out(0) = in0_shape0;\n",
    "            ''' # No need to add '@' when getting shape of tensors in C++.\n",
    "    )\n",
    "jt.flags.use_cuda = 1\n",
    "jt_int_mult(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f0cd3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jt.Var([5 2 1 4], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jittor as jt\n",
    "a = jt.Var([5,2,1,4])\n",
    "def jt_alias_test(a):\n",
    "    return jt.code(\n",
    "        a.shape,\n",
    "        a.dtype,\n",
    "        [a],\n",
    "        cpu_src=f'''\n",
    "            for (int i=0; i<in0_shape0; i++)\n",
    "                @out(i) = @in0(i);\n",
    "            ''' # No need to add '@' when getting shape of tensors in C++.\n",
    "    )\n",
    "jt.flags.use_cuda = 1\n",
    "jt_alias_test(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fcfcbe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jt.Var([5 2 1 4], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jittor as jt\n",
    "a = jt.Var([5,2,1,4])\n",
    "def jt_alias_test(a):\n",
    "    return jt.code(\n",
    "        a.shape,\n",
    "        a.dtype,\n",
    "        [a],\n",
    "        cpu_src=f'''\n",
    "            @alias(b, in0)\n",
    "            @alias(res, out)\n",
    "            for (int i=0; i<b_shape0; i++)\n",
    "                @res(i) = @b(i);\n",
    "            ''' # No need to add '@' when getting shape of tensors in C++.\n",
    "    )\n",
    "jt.flags.use_cuda = 1\n",
    "jt_alias_test(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\"Wrong inputs arguments, Please refer to examples(help(jt.numpy)).\\n\\nTypes of your inputs are:\\n self\\t= Var,\\n args\\t= (),\\n\\nThe function declarations are:\\n ArrayArgs fetch_sync()\\n\\nFailed reason:\\u001b[38;5;1m[f 0910 20:43:24.055093 84 parallel_compiler.cc:331] Error happend during compilation:\\n [Error] source file location:/home/a516/.cache/jittor/jt1.3.9/g++12.3.0/py3.8.20/Linux-6.6.87.2xef/13thGenIntelRCx37/4832/default/cu12.8.61/jit/code__IN_SIZE_1__in0_dim_4__in0_type_float32__OUT_SIZE_1__out0_dim_4__out0_type_int32__HEA___hash_1fefade715b2521e_op.cc\\nCompile operator(1/2)failed:Op(47:1:1:1:i1:o1:s0:g1,code->48)\\n\\nReason: \\u001b[38;5;1m[f 0910 20:43:24.054767 44:C1 op_compiler.cc:719] \\u001b[38;5;1m[f 0910 20:43:24.054761 44:C1 op_compiler.cc:719] \\u001b[38;5;1m[f 0910 20:43:24.054749 44:C1 op_compiler.cc:719] \\u001b[38;5;1m[f 0910 20:43:24.054727 44:C1 op_compiler.cc:687] Check failed macros.at(dim)(4) == S(args.size())(1) res dimension not matched\\u001b[m\\nJit compiler error:\\n                @res(i) = @b(i);\\u001b[m\\nJit compiler error:\\n    @CODE\\u001b[m\\nJit compiler error:\\n#ifndef JIT\\u001b[m\\n\\u001b[m\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\"Wrong inputs arguments, Please refer to examples(help(jt.numpy)).\\n\\nTypes of your inputs are:\\n self\\t= Var,\\n args\\t= (),\\n\\nThe function declarations are:\\n ArrayArgs fetch_sync()\\n\\nFailed reason:\\u001b[38;5;1m[f 0910 20:47:22.597627 48 parallel_compiler.cc:331] Error happend during compilation:\\n [Error] source file location:/home/a516/.cache/jittor/jt1.3.9/g++12.3.0/py3.8.20/Linux-6.6.87.2xef/13thGenIntelRCx37/4832/default/cu12.8.61/jit/code__IN_SIZE_1__in0_dim_4__in0_type_float32__OUT_SIZE_1__out0_dim_4__out0_type_int32__HEA___hash_99721febc08a03a_op.cc\\nCompile operator(1/2)failed:Op(28:1:1:1:i1:o1:s0:g1,code->29)\\n\\nReason: \\u001b[38;5;1m[f 0910 20:47:22.596386 40:C1 op_compiler.cc:719] \\u001b[38;5;1m[f 0910 20:47:22.596382 40:C1 op_compiler.cc:719] \\u001b[38;5;1m[f 0910 20:47:22.596373 40:C1 op_compiler.cc:719] \\u001b[38;5;1m[f 0910 20:47:22.596345 40:C1 op_compiler.cc:687] Check failed macros.at(dim)(4) == S(args.size())(1) out0 dimension not matched\\u001b[m\\nJit compiler error:\\n                @out0(i) = @in0(i);\\u001b[m\\nJit compiler error:\\n    @CODE\\u001b[m\\nJit compiler error:\\n#ifndef JIT\\u001b[m\\n\\u001b[m\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ad48a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jt.Var([5], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jittor as jt\n",
    "a = jt.rand([5,2,1,4])\n",
    "def jt_int_mult(a):\n",
    "    x = jt.Var([1])\n",
    "    return jt.code(\n",
    "        x.shape,\n",
    "        jt.int32,\n",
    "        [a],\n",
    "        cpu_src=f'''\n",
    "                @alias(a, in0)\n",
    "                @out(0) = a_shape0;\n",
    "            ''' # No need to add '@' when getting shape of tensors in C++.\n",
    "    )\n",
    "jt.flags.use_cuda = 1\n",
    "jt_int_mult(a)\n",
    "# 利用 alias 只能给输入变量取别名，不能给其维度等属性取别名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef97f325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jt.Var([5], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jittor as jt\n",
    "a = jt.rand([5,2,1,4])\n",
    "def jt_int_mult(a):\n",
    "    x = jt.Var([1])\n",
    "    return jt.code(\n",
    "        x.shape,\n",
    "        jt.int32,\n",
    "        [a],\n",
    "        cuda_src='''\n",
    "            __global__ void kernel_add(@ARGS_DEF) {\n",
    "                @PRECALC;\n",
    "                @alias(a, in0);\n",
    "                @out(0) = a_shape0;\n",
    "            }\n",
    "            kernel_add<<<32, 32>>>(@ARGS);\n",
    "            ''' # No need to add '@' when getting shape of tensors in C++.\n",
    "    )\n",
    "jt.flags.use_cuda = 1\n",
    "jt_int_mult(a)\n",
    "# 利用 alias 只能给输入变量取别名，不能给其维度等属性取别名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "febefbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling Operators(1/1) used: 2.13s eta:    0s \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jt.Var([5], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jittor as jt\n",
    "a = jt.rand([5,2,1,4])\n",
    "def jt_int_mult(a):\n",
    "    x = jt.Var([1])\n",
    "    return jt.code(\n",
    "        x.shape,\n",
    "        jt.int32,\n",
    "        [a],\n",
    "        cuda_src='''\n",
    "            __global__ void kernel_add(@ARGS_DEF) {\n",
    "                @PRECALC;\n",
    "                @alias(a, in0);\n",
    "                \n",
    "                const int c = a_shape0;\n",
    "                @out(0) = c;\n",
    "            }\n",
    "            kernel_add<<<32, 32>>>(@ARGS);\n",
    "            ''' # No need to add '@' when getting shape of tensors in C++.\n",
    "    )\n",
    "jt.flags.use_cuda = 1\n",
    "jt_int_mult(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c252ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jittor as jt\n",
    "def upfirdn2d_large(\n",
    "    input: jt.Var,    # 输入张量，形状：(major_dim, in_h, in_w, minor_dim)\n",
    "    kernel: jt.Var,   # 滤波核张量，形状：(kernel_h, kernel_w)\n",
    "    up_x: int,        # x方向上采样倍数\n",
    "    up_y: int,        # y方向上采样倍数\n",
    "    down_x: int,      # x方向下采样倍数\n",
    "    down_y: int,      # y方向下采样倍数\n",
    "    pad_x0: int,      # x方向左填充\n",
    "    pad_x1: int,      # x方向右填充\n",
    "    pad_y0: int,      # y方向上填充\n",
    "    pad_y1: int       # y方向下填充\n",
    ") -> jt.Var:\n",
    "    \"\"\"\n",
    "    Upfirdn2d impletement by jittor.code\n",
    "    \"\"\"\n",
    "    # -------------------------- 1. 计算输出张量形状（与原核函数逻辑一致）--------------------------\n",
    "    major_dim = input.shape[0]  # 对应 p.major_dim（如批量维度）\n",
    "    in_h = input.shape[1]       # 对应 p.in_h（输入高度）\n",
    "    in_w = input.shape[2]       # 对应 p.in_w（输入宽度）\n",
    "    minor_dim = input.shape[3]  # 对应 p.minor_dim（如通道维度）\n",
    "    kernel_h = kernel.shape[0]  # 对应 p.kernel_h（滤波核高度）\n",
    "    kernel_w = kernel.shape[1]  # 对应 p.kernel_w（滤波核宽度）\n",
    "\n",
    "    # 计算输出高度/宽度（原核函数 p.out_h/p.out_w 的计算公式）\n",
    "    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h + down_y) // down_y\n",
    "    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w + down_x) // down_x\n",
    "\n",
    "    # 定义线程配置参数（与原核函数一致：分块处理大任务）\n",
    "    loop_major = (major_dim - 1) // 16384 + 1  # 对应 p.loop_major\n",
    "    loop_x = 4                                 # 对应 p.loop_x\n",
    "    block_size_x = 4                           # 对应原 blockDim.x\n",
    "    block_size_y = 32                          # 对应原 blockDim.y\n",
    "\n",
    "\n",
    "    # -------------------------- 2. 调用 jt.code 实现 CUDA 逻辑--------------------------\n",
    "    return jt.code(\n",
    "        shape=(major_dim, out_h, out_w, minor_dim),  # 输出张量形状\n",
    "        dtype=input.dtype,                            # 输出数据类型与输入一致\n",
    "        # inputs = [@in0, @in1, @in2, @in3, @in4, @in5,   @in6,   @in7,    @in8,   @in9]\n",
    "        inputs=[input, kernel, jt.array([up_x], dtype=jt.int32), \n",
    "        jt.array([up_y], dtype=jt.int32),\n",
    "        jt.array([down_x], dtype=jt.int32),\n",
    "        jt.array([down_y], dtype=jt.int32),\n",
    "        jt.array([pad_x0], dtype=jt.int32),\n",
    "        jt.array([pad_x1], dtype=jt.int32),\n",
    "        jt.array([pad_y0], dtype=jt.int32),\n",
    "        jt.array([pad_y1], dtype=jt.int32)],\n",
    "        # CUDA API\n",
    "        cuda_header='''\n",
    "        #include <cuda.h>\n",
    "        #include <cuda_runtime.h>\n",
    "        ''',\n",
    "        cuda_src='''\n",
    "            __global__ static void jt_upfirdn2d_kernel(@ARGS_DEF) {\n",
    "                @PRECALC\n",
    "                @alias(input, in0);\n",
    "                @alias(kernel, in1);\n",
    "                @alias(up_x, in2);\n",
    "                @alias(up_y, in3);\n",
    "                @alias(down_x, in4);\n",
    "                @alias(down_y, in5);\n",
    "                @alias(pad_x0, in6);\n",
    "                @alias(pad_x1, in7);\n",
    "                @alias(pad_y0, in8);\n",
    "                @alias(pad_y1, in9);\n",
    "\n",
    "                const int major_dim = input_shape0;\n",
    "                const int in_h = input_shape1;\n",
    "                const int in_w = input_shape2;\n",
    "                const int minor_dim = input_shape3;\n",
    "                const int kernel_h = kernel_shape0;\n",
    "                const int kernel_w = kernel_shape1;\n",
    "                int out_h = (in_h * @up_y(0) + @pad_y0(0) + @pad_y1(0) - kernel_shape0 + @down_y(0)) / @down_y(0);\n",
    "                int out_w = (in_w * @up_x(0) + @pad_x0(0) + @pad_x1(0) - kernel_shape1 + @down_x(0)) / @down_x(0);\n",
    "                const int res_int = (major_dim - 1) / 16384;\n",
    "                const int loop_major = res_int + 1;\n",
    "                const int loop_x = 4;\n",
    "\n",
    "                int minor_idx = blockIdx.x * blockDim.x + threadIdx.x;  // 对应原 minor_idx\n",
    "                int out_y = minor_idx / minor_dim;                     // 对应原 out_y\n",
    "                minor_idx -= out_y * minor_dim;                        // 修正 minor_idx\n",
    "\n",
    "                int out_x_base = blockIdx.y * loop_x * blockDim.y + threadIdx.y;  // 对应原 out_x_base\n",
    "                int major_idx_base = blockIdx.z * loop_major;                    // 对应原 major_idx_base\n",
    "\n",
    "                // 边界检查：超出输出范围的线程直接退出（避免无效计算）\n",
    "                if (out_x_base >= out_w || out_y >= out_h || major_idx_base >= major_dim) {\n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                int mid_y = out_y * @down_y(0) + @up_y(0) - 1 - @pad_y0(0);  // 对应原 mid_y\n",
    "                \n",
    "                int in_y = min(max((mid_y >= 0 ? mid_y / @up_y(0) : (mid_y - @up_y(0) + 1) / @up_y(0)), 0), in_h);  // 对应原 floor_div\n",
    "                \n",
    "                int h = min(max((mid_y + kernel_h >= 0 ? (mid_y + kernel_h) / @up_y(0) : ((mid_y + kernel_h) - @up_y(0) + 1) / @up_y(0)), 0), in_h) - in_y;  // 对应原 h\n",
    "                \n",
    "                int kernel_y = mid_y + kernel_h - (in_y + 1) * @up_y(0);  // 对应原 kernel_y\n",
    "\n",
    "                // 遍历批量维度（major_dim）：分 loop_major 次处理\n",
    "                for (int loop_major_cnt = 0, major_idx = major_idx_base;\n",
    "                     loop_major_cnt < loop_major && major_idx < major_dim;\n",
    "                     loop_major_cnt++, major_idx++) {\n",
    "                    \n",
    "                    // 遍历 x 方向：分 loop_x 次处理（每次处理 blockDim.y 个 out_x）\n",
    "                    for (int loop_x_cnt = 0, out_x = out_x_base;\n",
    "                         loop_x_cnt < loop_x && out_x < out_w;\n",
    "                         loop_x_cnt++, out_x += blockDim.y) {\n",
    "\n",
    "                        int mid_x = out_x * @down_x(0) + @up_x(0) - 1 - @pad_x0(0);  // 对应原 mid_x\n",
    "                        int in_x = min(max((mid_x >= 0 ? mid_x / @up_x(0) : (mid_x - @up_x(0) + 1) / @up_x(0)), 0), in_w);  // 对应原 floor_div\n",
    "                        int w = min(max((mid_x + kernel_w >= 0 ? (mid_x + kernel_w) / @up_x(0) : ((mid_x + kernel_w) - @up_x(0) + 1) / @up_x(0)), 0), in_w) - in_x;  // 对应原 w\n",
    "                        int kernel_x = mid_x + kernel_w - (in_x + 1) * @up_x(0);  // 对应原 kernel_x\n",
    "\n",
    "                        float val = 0.0f;  // 存储当前输出像素的累加值\n",
    "\n",
    "                        // 遍历滤波核 y 方向覆盖的输入行\n",
    "                        for (int y = 0; y < h; y++) {\n",
    "                            // 遍历滤波核 x 方向覆盖的输入列\n",
    "                            for (int x = 0; x < w; x++) {\n",
    "                                // 1. 读取输入张量对应位置的值（@in0 访问 input，索引：(major_idx, in_y+y, in_x+x, minor_idx)）\n",
    "                                float input_val = @in0(major_idx, in_y + y, in_x + x, minor_idx);\n",
    "                                // 2. 读取滤波核对应位置的权重（@in1 访问 kernel，索引：(kernel_y+y, kernel_x+x)）\n",
    "                                float kernel_val = @in1(kernel_y + y, kernel_x + x);\n",
    "                                // 3. 累加：输入值 × 核权重\n",
    "                                val += input_val * kernel_val;\n",
    "                            }\n",
    "                        }\n",
    "                        @out(major_idx, out_y, out_x, minor_idx) = val;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "           jt_upfirdn2d_kernel<<<32, 32>>>(@ARGS);\n",
    "        '''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e21937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出张量形状: (1, 8, 8, 1)\n",
      "输出张量前4x4区域:\n",
      " [[0.11111111 0.22222222 0.11111111 0.22222222]\n",
      " [0.11111111 0.11111111 0.11111111 0.11111111]\n",
      " [0.11111111 0.22222222 0.11111111 0.22222222]\n",
      " [0.11111111 0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "import numpy as np\n",
    "\n",
    "jt.flags.use_cuda = 1\n",
    "\n",
    "# -------------------------- 1. 初始化输入张量 --------------------------\n",
    "batch, in_h, in_w, channels = 1, 4, 4, 1  # 小尺寸，便于验证\n",
    "input_np = np.ones((batch, in_h, in_w, channels), dtype=np.float32)\n",
    "input_jt = jt.array(input_np)\n",
    "\n",
    "# 卷积核全 1，3x3\n",
    "kernel_np = np.ones((3,3), dtype=np.float32)\n",
    "kernel_np /= kernel_np.sum()  # 归一化\n",
    "kernel_jt = jt.array(kernel_np)\n",
    "\n",
    "# -------------------------- 2. 设置采样/填充参数 --------------------------\n",
    "up_x, up_y = 2, 2\n",
    "down_x, down_y = 1, 1\n",
    "pad_x0, pad_x1 = 1, 1\n",
    "pad_y0, pad_y1 = 1, 1\n",
    "\n",
    "# -------------------------- 3. 调用 jt_upfirdn2d_large --------------------------\n",
    "output = upfirdn2d_large(\n",
    "    input=input_jt,\n",
    "    kernel=kernel_jt,\n",
    "    up_x=up_x,\n",
    "    up_y=up_y,\n",
    "    down_x=down_x,\n",
    "    down_y=down_y,\n",
    "    pad_x0=pad_x0,\n",
    "    pad_x1=pad_x1,\n",
    "    pad_y0=pad_y0,\n",
    "    pad_y1=pad_y1\n",
    ")\n",
    "\n",
    "# -------------------------- 4. 转回 NumPy 查看结果 --------------------------\n",
    "output_np = output.numpy()\n",
    "print(\"输出张量形状:\", output_np.shape)\n",
    "print(\"输出张量前4x4区域:\\n\", output_np[0, :4, :4, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac9bf1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost:\n",
      " new_fib:5.239100028120447e-05 \n",
      " fibonacci:8.127972070999022\n",
      "new_fib is faster.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "def fibonacci(n):\n",
    "    if n == 1:\n",
    "        return 1\n",
    "    elif n == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return fibonacci(n - 1) + fibonacci(n - 2)\n",
    "@lru_cache(maxsize=None)\n",
    "def new_fib(n):\n",
    "    if n == 1:\n",
    "        return 1\n",
    "    elif n == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return new_fib(n - 1) + new_fib(n - 2)\n",
    "n = 40\n",
    "t1 = time.perf_counter()\n",
    "fibonacci(n)\n",
    "t2 = time.perf_counter()\n",
    "new_fib(n)\n",
    "t3 = time.perf_counter()\n",
    "print(f'Time cost:\\n new_fib:{t3 - t2} \\n fibonacci:{t2 - t1}')\n",
    "if t2 - t1 > t3 - t2:\n",
    "    print(\"new_fib is faster.\")\n",
    "elif t2 - t1 < t3 - t2:\n",
    "    print(\"fibonacci is faster.\")\n",
    "else:\n",
    "    print(\"Can not dicide which is faster in this situation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jittor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
